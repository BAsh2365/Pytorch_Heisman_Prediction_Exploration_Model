# -*- coding: utf-8 -*-
"""CFB Heisman Predictor exploration

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CIb1I3iXs9oqakao8vLcsxBpFee_xTCI
"""

!pip install torch pandas matplotlib seaborn scikit-learn

#install packages and load them
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder

# Set random seeds for reproducibility
np.random.seed(42)
torch.manual_seed(42)

print(f"PyTorch version: {torch.__version__}")
print(f"GPU available: {torch.cuda.is_available()}")

df = pd.read_csv('heisman_finalists_complete_2000_2025_all.csv')

print(df.head())

df_winners = df[df['won_heisman'] == 1].copy() #adding feature columns for winners/runner ups (binary classification)

#categorical variables encoding
le_position = LabelEncoder()
le_conference = LabelEncoder()
le_class = LabelEncoder()

# Fit them on the data
df_winners['position_encoded'] = le_position.fit_transform(df_winners['position'].fillna('N/A'))
df_winners['conference_encoded'] = le_conference.fit_transform(df_winners['conference'].fillna('independent'))
df_winners['class_encoded'] = le_class.fit_transform(df_winners['class'].fillna('Senior'))


print(f'Position Encoding Mapping: {list(le_position.classes_)}')
print(f'Conference Encoding Mapping: {list(le_conference.classes_)}')
print(f'Class Encoding Mapping: {list(le_class.classes_)}')

"""The output above shows the mapping from the actual position names to their encoded numerical values. For example, the first element in that list corresponds to `0`, the second to `1`, and so on."""

# INPUT FEATURES (what we know)
input_features = [
    'position_encoded',
    'conference_encoded',
    'class_encoded',
    'team_wins',
    'team_losses',
    'year',
    'win_percentage',
    'efficiency_score', #efficiency_score = (total_offense / games_total) + (total_tds * 50) made by claude 4.5 sonnet
    'td_int_ratio'
]

# OUTPUT TARGETS (what we want to predict)
output_targets = [
    'total_offense',
    'total_tds',
    'passing_tds',
    'rushing_tds',
    'yards_per_game',
    'receiving_tds',
    'receptions'
]

# Prepare arrays
X = df_winners[input_features].fillna(0).values
y = df_winners[output_targets].fillna(0).values

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Normalize (important for neural networks!)
scaler_X = StandardScaler()
scaler_y = StandardScaler()

X_train = scaler_X.fit_transform(X_train)
X_test = scaler_X.transform(X_test)
y_train = scaler_y.fit_transform(y_train)
y_test = scaler_y.transform(y_test)

# Convert to PyTorch tensors
X_train = torch.FloatTensor(X_train)
y_train = torch.FloatTensor(y_train)
X_test = torch.FloatTensor(X_test)
y_test = torch.FloatTensor(y_test)

class HeismanDataset(Dataset):
    def __init__(self, features, targets):
        self.features = features
        self.targets = targets

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return self.features[idx], self.targets[idx]

train_dataset = HeismanDataset(X_train, y_train)
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)

class HeismanPredictor(nn.Module):
  #3 layer NN model with a dropout rate until last layer

    def __init__(self):
        super(HeismanPredictor, self).__init__()

        self.layer1 = nn.Linear(9, 32) #inputs
        self.layer2 = nn.Linear(32, 16) #NN layer (16 neuron Lienar NN)
        self.layer3 = nn.Linear(16, 7) #outputs

        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.2)

    def forward(self, x):
        x = self.relu(self.layer1(x))
        x = self.dropout(x)

        x = self.relu(self.layer2(x))
        x = self.dropout(x)

        x = self.layer3(x)  #no dropout needed for lin reg (end of model)
        return x

def forward(self, x):
        # Forward pass through layers
        x = self.relu(self.layer1(x))
        x = self.dropout(x)

        x = self.relu(self.layer2(x))
        x = self.dropout(x)

        x = self.layer3(x)  # No activation on output for regression
        return x

# Create model
model = HeismanPredictor()

# Mean Squared Error for regression
criterion = nn.MSELoss()

optimizer = optim.Adam(model.parameters(), lr=0.001) #adam (adaptive learning rate, 0.001 is a good constant number)

num_epochs = 350
losses = [] # Initialize list to store training loss values
val_losses = [] # Initialize list to store validation loss values

for epoch in range(num_epochs):
    model.train()  # Set to training mode

    for batch_X, batch_y in train_loader:
        # 1. Forward pass
        predictions = model(batch_X)

        # 2. Calculate loss
        loss = criterion(predictions, batch_y)

        # 3. Backward pass
        optimizer.zero_grad()  # gradient descent
        loss.backward()        # back propigation (linear alg)

        # 4. Update weights (step function)
        optimizer.step()

    losses.append(loss.item()) # Store the training loss for this epoch

    # Validation phase
    model.eval()  # Set to evaluation mode
    with torch.no_grad(): # Disable gradient calculations for no memeory leaks
        val_predictions = model(X_test)
        val_loss = criterion(val_predictions, y_test)
    val_losses.append(val_loss.item()) # Store the validation loss for this epoch

    model.train() # Set back to training mode for the next epoch

    if (epoch + 1) % 50 == 0:
        print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {loss.item():.4f}, Validation Loss: {val_loss.item():.4f}')

model.eval()

new_player = np.array([[
    np.random.uniform(0,9),     #random position (all offensive stats based on majority winners)
    np.random.uniform(0,6),  #conference (random, though big 10 is the majority)
    np.random.uniform(0,5),      # class_encoded
    np.random.randint(1, 13),     # team_wins (random between 1 and 12, inclusive)
    np.random.randint(0, 8),      # team_losses
    2026,                         # year
    np.random.uniform(0.5, 1.0),  # win_percentage (random float between 0.5 and 1.0)
    np.random.uniform(1000, 3000),# efficiency_score (random float)
    np.random.uniform(0.5, 5.0)   # td_int_ratio (random float)
]])

'''
    You ALWAYS have to remember that lists start at zero.
    Based on the encodings we've checked above these randomizations should be accurate.
    Also note inclusive random integers.
    input_features = [
    'position_encoded',
    'conference_encoded',
    'class_encoded',
    'team_wins',
    'team_losses',
    'year',
    'win_percentage',
    'efficiency_score', #efficiency_score = (total_offense / games_total) + (total_tds * 50) made by claude 4.5 sonnet
    'td_int_ratio'
]

   '''

# Scale input
new_player_scaled = scaler_X.transform(new_player)
new_player_tensor = torch.FloatTensor(new_player_scaled)

# Predict
with torch.no_grad():
    prediction_scaled = model(new_player_tensor)
    # Inverse transform to get actual values
    prediction = scaler_y.inverse_transform(prediction_scaled.numpy())

# Get the actual position name using inverse_transform
predicted_position_encoded = int(new_player[0, input_features.index('position_encoded')])
predicted_position_name = le_position.inverse_transform([predicted_position_encoded])[0]


# Get the actual conference
predicted_conf_encoded = int(new_player[0, input_features.index('conference_encoded')])
predicted_conf_name = le_conference.inverse_transform([predicted_conf_encoded])[0]

# Display results
print("\nPredicted Stats for the next heisman winner (offensive player)")
print(f"Predicted Position: {predicted_position_name}")
print(f"Predicted Conference: {predicted_conf_name}")
print(f"  Total Offense: {prediction[0, 0]:.0f} yards")
print(f"  Total TDs: {prediction[0, 1]:.0f}")
print(f"  Passing TDs: {prediction[0, 2]:.0f}")
print(f"  Rushing TDs: {prediction[0, 3]:.0f}")
print(f"  Yards per Game: {prediction[0, 4]:.0f}")
print(f"  Receiving TDs: {prediction[0, 5]:.0f}")
print(f"  Receptions: {prediction[0, 6]:.0f}")

"""**Reasoning**:
The previous code block calculated and stored both training and validation losses in the `losses` and `val_losses` lists, respectively. Now, I need to modify the plotting cell to display both of these on separate subplots to visualize the model's performance during training. The original plotting code has a `NameError` which needs to be corrected as well.


"""

import matplotlib.pyplot as plt

# Create figure with subplots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Plot 1: Training loss over time
ax1.plot(losses, label='Training Loss')
ax1.set_xlabel('Epoch')
ax1.set_ylabel('Loss (MSE)')
ax1.set_title('Training Loss Over Time')
ax1.legend()
ax1.grid(True)

# Plot 2: Validation loss over time
ax2.plot(val_losses, label='Validation Loss', color='orange')
ax2.set_xlabel('Epoch')
ax2.set_ylabel('Loss (MSE)')
ax2.set_title('Validation Loss Over Time')
ax2.legend()
ax2.grid(True)

plt.tight_layout()
plt.show()